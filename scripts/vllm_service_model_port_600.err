Found ulimit of 10240 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[2025-05-12 04:55:33] INFO core.py:58: Initializing a V1 LLM engine (v0.8.5.post1) with config: model='deepseek-ai/deepseek-moe-16b-chat', speculative_config=None, tokenizer='deepseek-ai/deepseek-moe-16b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/deepseek-moe-16b-chat, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
[2025-05-12 04:55:33] WARNING utils.py:2522: Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5c67422dd0>
[2025-05-12 04:55:34] INFO parallel_state.py:1004: rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[2025-05-12 04:55:34] INFO cuda.py:221: Using Flash Attention backend on V1 engine.
[2025-05-12 04:55:34] WARNING topk_topp_sampler.py:69: FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[2025-05-12 04:55:34] INFO gpu_model_runner.py:1329: Starting to load model deepseek-ai/deepseek-moe-16b-chat...
[2025-05-12 04:55:34] WARNING config.py:4122: `torch.compile` is turned on, but the model deepseek-ai/deepseek-moe-16b-chat does not support it. Please open an issue on GitHub if you want it to be supported.
[2025-05-12 04:55:35] INFO weight_utils.py:265: Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:05,  1.03it/s]
Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:02<00:05,  1.13s/it]
Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:03<00:04,  1.19s/it]
Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:04<00:03,  1.23s/it]
Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:05<00:02,  1.08s/it]
Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:06<00:01,  1.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.13s/it]

[2025-05-12 04:55:44] INFO loader.py:458: Loading weights took 8.24 seconds
[2025-05-12 04:55:44] INFO gpu_model_runner.py:1347: Model loading took 30.5301 GiB and 9.766635 seconds
[2025-05-12 04:55:44] WARNING fused_moe.py:668: Using default MoE config. Performance might be sub-optimal! Config file not found at /home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1408,device_name=NVIDIA_A100_80GB_PCIe.json
[2025-05-12 04:55:45] INFO kv_cache_utils.py:634: GPU KV cache size: 136,704 tokens
[2025-05-12 04:55:45] INFO kv_cache_utils.py:637: Maximum concurrency for 4,096 tokens per request: 33.38x
[2025-05-12 04:55:49] INFO gpu_model_runner.py:1686: Graph capturing finished in 4 secs, took 0.12 GiB
[2025-05-12 04:55:49] INFO core.py:159: init engine (profile, create kv cache, warmup model) took 5.27 seconds
[2025-05-12 04:55:49] INFO core_client.py:439: Core engine process 0 ready.
[2025-05-12 04:55:50] INFO api_server.py:1090: Starting vLLM API server on http://0.0.0.0:8001
[2025-05-12 04:55:50] INFO launcher.py:28: Available routes are:
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /openapi.json, Methods: HEAD, GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /docs, Methods: HEAD, GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /redoc, Methods: HEAD, GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /health, Methods: GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /load, Methods: GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /ping, Methods: GET, POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /tokenize, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /detokenize, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v1/models, Methods: GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /version, Methods: GET
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v1/chat/completions, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v1/completions, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v1/embeddings, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /pooling, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /score, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v1/score, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v1/audio/transcriptions, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /rerank, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v1/rerank, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /v2/rerank, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /invocations, Methods: POST
[2025-05-12 04:55:50] INFO launcher.py:36: Route: /metrics, Methods: GET
INFO:     Started server process [2538483]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
[2025-05-12 04:56:16] INFO chat_utils.py:397: Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2025-05-12 04:56:16] ERROR serving_chat.py:200: Error in preprocessing prompt inputs
Traceback (most recent call last):
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
    ) = await self._preprocess_chat(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 451, in _preprocess_chat
    prompt_inputs = await self._tokenize_prompt_input_async(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 281, in _tokenize_prompt_input
    return next(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 304, in _tokenize_prompt_inputs
    yield self._normalize_prompt_text_to_input(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 196, in _normalize_prompt_text_to_input
    return self._validate_input(request, input_ids, input_text)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_engine.py", line 259, in _validate_input
    raise ValueError(
ValueError: This model's maximum context length is 4096 tokens. However, you requested 6171 tokens (5471 in the messages, 700 in the completion). Please reduce the length of the messages or completion.
slurmstepd-ki-g0002: error: *** JOB 600 ON ki-g0002 CANCELLED AT 2025-05-12T04:57:36 ***
[2025-05-12 04:57:36] INFO launcher.py:79: Shutting down FastAPI HTTP server.
