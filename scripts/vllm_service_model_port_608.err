Found ulimit of 10240 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=4096 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.
[2025-05-12 05:30:27] INFO core.py:58: Initializing a V1 LLM engine (v0.8.5.post1) with config: model='deepseek-ai/deepseek-moe-16b-chat', speculative_config=None, tokenizer='deepseek-ai/deepseek-moe-16b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/deepseek-moe-16b-chat, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
[2025-05-12 05:30:27] WARNING utils.py:2522: Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff41de1edd0>
[2025-05-12 05:30:27] INFO parallel_state.py:1004: rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[2025-05-12 05:30:27] INFO cuda.py:221: Using Flash Attention backend on V1 engine.
[2025-05-12 05:30:27] WARNING topk_topp_sampler.py:69: FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[2025-05-12 05:30:27] INFO gpu_model_runner.py:1329: Starting to load model deepseek-ai/deepseek-moe-16b-chat...
[2025-05-12 05:30:28] WARNING config.py:4122: `torch.compile` is turned on, but the model deepseek-ai/deepseek-moe-16b-chat does not support it. Please open an issue on GitHub if you want it to be supported.
[2025-05-12 05:30:28] INFO weight_utils.py:265: Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:05,  1.09it/s]
Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:02<00:05,  1.11s/it]
Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:03<00:04,  1.17s/it]
Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:04<00:03,  1.22s/it]
Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:05<00:02,  1.08s/it]
Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:06<00:01,  1.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.12s/it]

[2025-05-12 05:30:37] INFO loader.py:458: Loading weights took 8.20 seconds
[2025-05-12 05:30:37] INFO gpu_model_runner.py:1347: Model loading took 30.5301 GiB and 9.220625 seconds
[2025-05-12 05:30:37] WARNING fused_moe.py:668: Using default MoE config. Performance might be sub-optimal! Config file not found at /home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1408,device_name=NVIDIA_A100_80GB_PCIe.json
[2025-05-12 05:30:38] INFO kv_cache_utils.py:634: GPU KV cache size: 136,560 tokens
[2025-05-12 05:30:38] INFO kv_cache_utils.py:637: Maximum concurrency for 16,384 tokens per request: 8.33x
[2025-05-12 05:30:42] INFO gpu_model_runner.py:1686: Graph capturing finished in 4 secs, took 0.12 GiB
[2025-05-12 05:30:42] INFO core.py:159: init engine (profile, create kv cache, warmup model) took 5.23 seconds
[2025-05-12 05:30:42] INFO core_client.py:439: Core engine process 0 ready.
[2025-05-12 05:30:42] INFO api_server.py:1090: Starting vLLM API server on http://0.0.0.0:8001
[2025-05-12 05:30:42] INFO launcher.py:28: Available routes are:
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /openapi.json, Methods: GET, HEAD
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /docs, Methods: GET, HEAD
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /docs/oauth2-redirect, Methods: GET, HEAD
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /redoc, Methods: GET, HEAD
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /health, Methods: GET
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /load, Methods: GET
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /ping, Methods: GET, POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /tokenize, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /detokenize, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v1/models, Methods: GET
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /version, Methods: GET
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v1/chat/completions, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v1/completions, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v1/embeddings, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /pooling, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /score, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v1/score, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v1/audio/transcriptions, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /rerank, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v1/rerank, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /v2/rerank, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /invocations, Methods: POST
[2025-05-12 05:30:42] INFO launcher.py:36: Route: /metrics, Methods: GET
INFO:     Started server process [2564032]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
[2025-05-12 05:31:49] INFO chat_utils.py:397: Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2025-05-12 05:31:49] INFO logger.py:39: Received request chatcmpl-2600880d915748068df92e87f9ae8096: prompt: "<｜begin▁of▁sentence｜>User: Summarize the following code changes from commit eba4527. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/CUM_report/commit_log.tex b/CUM_report/commit_log.tex\nindex f5c5b46..2a79aab 100644\n--- a/CUM_report/commit_log.tex\n+++ b/CUM_report/commit_log.tex\n@@ -1,78 +1,50 @@\n \\documentclass{article}\n \\usepackage[utf8]{inputenc}\n-\\title{Commit Summary Report}\n-\\author{Generated by CUM Report}\n+\\usepackage{parskip}\n+\\usepackage{hyperref}\n+\\usepackage[T1]{fontenc}\n+\\usepackage{lmodern} \n+\\usepackage{amsmath} \n+\\usepackage{courier} \n+\n+\\title{Commit Summary Report (CUM Report)}\n+\\author{Generated by Git Post-Commit Hook}\n \\date{\\today}\n+\n \\begin{document}\n \\maketitle\n+\\begin{abstract}\n+This document contains summaries of commits, generated automatically after each commit and included herein.\n+\\end{abstract}\n \\tableofcontents\n-\\section{Introduction}\n-This document contains summaries of commits in this repository.\n-section*{Commit c69d48e by Anderson-L-Luiz (2025-05-11 18:08:09 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes from commit c69d48e implement a Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report. The hook is triggered before each `git push` and ensures that a complete and recent report is available.\n-\n-The hook is designed to work with vLLM (a virtual language model) for generating summaries of commit changes. The script first installs the necessary packages (including vLLM's API client), then checks that the required directories and files exist. It then creates the hook file and installs the pre--push hook.\n-\n-The script proceeds to configure and run the vLLM API client to generate a summary for the latest commit. The summary is then inserted into the LaTeX report, and any error messages or issues are logged.\n-\n-Finally, the script ensures that the hook has been set up correctly and exits with a success status.\n+\\newpage\n \n-The changes from the previous commit (bc90636) seem to be related to installation of required packages, creating required directories and files, and setting up and running the Git pre--push hook.\n-}\n-subsubsection*{Commit Details}\n-egin{itemize}\n-    item \textbf{Commit Hash:} c69d48e384e8ddfb4b33d0bc5d6a09646a809be9\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:08:09 +0000\n-end{itemize}\n-hrulefill\n+\\section{Introduction}\n+This document provides a log of commit summaries. Each section corresponds to a summary generated at the time of a commit.\n+The summaries are generated by an AI model based on the diff of the commit.\n \n-section*{Commit 07a68b7 by Anderson-L-Luiz (2025-05-11 18:09:36 +0000)}\n+section*{Commit 9b359d0 by Anderson-L-Luiz (2025-05-12 05:14:57 +0000)}\n subsubsection*{AI Generated Summary}\n {ontfamily{pcr}selectfont\n- The code changes from commit `c69d48e` in the provided diff likely achieve the following at a high level, summarizing the commits history:\n-\n-In summary:\n-\n--- The previous commit (`bc90636`) introduces the need for creating directories and files necessary for the script.\n--- The latest commit (`c69d48e`) fulfills this need by setting up and installing required packages, creating necessary directories and files, and setting up and running the Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report.\n-\n-These changes likely have a high impact as they improve the functionality of the commit--log.tex file. This kind of pre--push hook could automate the process of creating a summary of the latest commit and saving it in a LaTeX report, streamlining the process of maintaining a commit log and potentially contributing to better record--keeping and collaboration within the team.\n+ The commit c69d48e and its associated changes seem to have added functionality to automatically generate and include commit summaries in the commit\textbackslash{}_log.tex LaTeX file whenever a new commit is pushed to the repository. This feature is achieved in a post--commit Git hook script which triggers before the git push operation.\n \n-The purpose of the changes could be any combination of the following:\n--- Automating the process of creating a summary of the latest commit and incorporating it into the LaTeX report.\n--- Creating log entries more efficiently, such as by pre--populating logs with automatically generated summaries.\n--- Monitoring changes to a software project by creating a record of each subtile change and contributing to version control.\n--- Ensuring that each commit can be tracked with a recent summary added to the log. This ensures that the logs are up--to--date.\n+The script seems to be designed to:\n \n-Hence, the changes seem to aim at generating a summary of the latest commit and adding it to the LaTeX report in a Git repository. It does this by making use of a vLLM API for generating these summaries, and the script also checks for any issues and logs error messages. Thus, the changes likely aim to create a more efficient and automated way of managing a commit log.\n-}\n-subsubsection*{Commit Details}\n-egin\n\n[Diff truncated due to length]\n\nAssistant:", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:31:49] INFO async_llm.py:252: Added request chatcmpl-2600880d915748068df92e87f9ae8096.
[2025-05-12 05:31:53] INFO loggers.py:111: Engine 000: Avg prompt throughput: 140.8 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
[2025-05-12 05:32:03] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:32:13] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:35:20] INFO logger.py:39: Received request chatcmpl-915b4a4732874c6f8a9a7ad882451e08: prompt: "<｜begin▁of▁sentence｜>User: Summarize the following code changes from commit bb19cc4. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/CUM_report/commit_log.tex b/CUM_report/commit_log.tex\nindex f5c5b46..7143ba5 100644\n--- a/CUM_report/commit_log.tex\n+++ b/CUM_report/commit_log.tex\n@@ -1,79 +1,26 @@\n \\documentclass{article}\n \\usepackage[utf8]{inputenc}\n-\\title{Commit Summary Report}\n-\\author{Generated by CUM Report}\n+\\usepackage{parskip}\n+\\usepackage{hyperref}\n+\\usepackage[T1]{fontenc}\n+\\usepackage{lmodern}\n+\\usepackage{amsmath}\n+\\usepackage{courier} % For pcr font family\n+\n+\\title{Commit Summary Report (CUM Report)}\n+\\author{Generated by Git Post-Commit Hook}\n \\date{\\today}\n+\n \\begin{document}\n \\maketitle\n+\\begin{abstract}\n+This document contains summaries of commits, generated automatically after each commit and included herein.\n+\\end{abstract}\n \\tableofcontents\n-\\section{Introduction}\n-This document contains summaries of commits in this repository.\n-section*{Commit c69d48e by Anderson-L-Luiz (2025-05-11 18:08:09 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes from commit c69d48e implement a Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report. The hook is triggered before each `git push` and ensures that a complete and recent report is available.\n-\n-The hook is designed to work with vLLM (a virtual language model) for generating summaries of commit changes. The script first installs the necessary packages (including vLLM's API client), then checks that the required directories and files exist. It then creates the hook file and installs the pre--push hook.\n-\n-The script proceeds to configure and run the vLLM API client to generate a summary for the latest commit. The summary is then inserted into the LaTeX report, and any error messages or issues are logged.\n-\n-Finally, the script ensures that the hook has been set up correctly and exits with a success status.\n-\n-The changes from the previous commit (bc90636) seem to be related to installation of required packages, creating required directories and files, and setting up and running the Git pre--push hook.\n-}\n-subsubsection*{Commit Details}\n-egin{itemize}\n-    item \textbf{Commit Hash:} c69d48e384e8ddfb4b33d0bc5d6a09646a809be9\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:08:09 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit 07a68b7 by Anderson-L-Luiz (2025-05-11 18:09:36 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes from commit `c69d48e` in the provided diff likely achieve the following at a high level, summarizing the commits history:\n-\n-In summary:\n+\\newpage\n \n--- The previous commit (`bc90636`) introduces the need for creating directories and files necessary for the script.\n--- The latest commit (`c69d48e`) fulfills this need by setting up and installing required packages, creating necessary directories and files, and setting up and running the Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report.\n-\n-These changes likely have a high impact as they improve the functionality of the commit--log.tex file. This kind of pre--push hook could automate the process of creating a summary of the latest commit and saving it in a LaTeX report, streamlining the process of maintaining a commit log and potentially contributing to better record--keeping and collaboration within the team.\n-\n-The purpose of the changes could be any combination of the following:\n--- Automating the process of creating a summary of the latest commit and incorporating it into the LaTeX report.\n--- Creating log entries more efficiently, such as by pre--populating logs with automatically generated summaries.\n--- Monitoring changes to a software project by creating a record of each subtile change and contributing to version control.\n--- Ensuring that each commit can be tracked with a recent summary added to the log. This ensures that the logs are up--to--date.\n-\n-Hence, the changes seem to aim at generating a summary of the latest commit and adding it to the LaTeX report in a Git repository. It does this by making use of a vLLM API for generating these summaries, and the script also checks for any issues and logs error messages. Thus, the changes likely aim to create a more efficient and automated way of managing a commit log.\n-}\n-subsubsection*{Commit Details}\n-egin{itemize}\n-    item \textbf{Commit Hash:} 07a68b778a1003798118fa8e7833f5fd5a2947d4\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:09:36 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit f952688 by Anderson-L-Luiz (2025-05-11 18:22:43 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes in the commit f952688 for the file install_cum.sh likely modify the script's behavior upon completion.\n-\n-Before the changes, the script used an exit 0 command to indicate that the script had finished executing successfully, which may have been considered a standard way of indicating successful co\n\n[Diff truncated due to length]\n\nAssistant:", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:35:20] INFO async_llm.py:252: Added request chatcmpl-915b4a4732874c6f8a9a7ad882451e08.
[2025-05-12 05:35:23] INFO loggers.py:111: Engine 000: Avg prompt throughput: 149.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[2025-05-12 05:35:33] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 32.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[2025-05-12 05:35:43] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:35:53] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:40:59] INFO logger.py:39: Received request chatcmpl-3b6f0a2a3cab4ed790c5c093c454d9aa: prompt: "<｜begin▁of▁sentence｜>User: Summarize the following code changes from commit 1eff7f3. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/CUM_report/commit_log.tex b/CUM_report/commit_log.tex\nindex f5c5b46..7143ba5 100644\n--- a/CUM_report/commit_log.tex\n+++ b/CUM_report/commit_log.tex\n@@ -1,79 +1,26 @@\n \\documentclass{article}\n \\usepackage[utf8]{inputenc}\n-\\title{Commit Summary Report}\n-\\author{Generated by CUM Report}\n+\\usepackage{parskip}\n+\\usepackage{hyperref}\n+\\usepackage[T1]{fontenc}\n+\\usepackage{lmodern}\n+\\usepackage{amsmath}\n+\\usepackage{courier} % For pcr font family\n+\n+\\title{Commit Summary Report (CUM Report)}\n+\\author{Generated by Git Post-Commit Hook}\n \\date{\\today}\n+\n \\begin{document}\n \\maketitle\n+\\begin{abstract}\n+This document contains summaries of commits, generated automatically after each commit and included herein.\n+\\end{abstract}\n \\tableofcontents\n-\\section{Introduction}\n-This document contains summaries of commits in this repository.\n-section*{Commit c69d48e by Anderson-L-Luiz (2025-05-11 18:08:09 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes from commit c69d48e implement a Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report. The hook is triggered before each `git push` and ensures that a complete and recent report is available.\n-\n-The hook is designed to work with vLLM (a virtual language model) for generating summaries of commit changes. The script first installs the necessary packages (including vLLM's API client), then checks that the required directories and files exist. It then creates the hook file and installs the pre--push hook.\n-\n-The script proceeds to configure and run the vLLM API client to generate a summary for the latest commit. The summary is then inserted into the LaTeX report, and any error messages or issues are logged.\n-\n-Finally, the script ensures that the hook has been set up correctly and exits with a success status.\n-\n-The changes from the previous commit (bc90636) seem to be related to installation of required packages, creating required directories and files, and setting up and running the Git pre--push hook.\n-}\n-subsubsection*{Commit Details}\n-egin{itemize}\n-    item \textbf{Commit Hash:} c69d48e384e8ddfb4b33d0bc5d6a09646a809be9\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:08:09 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit 07a68b7 by Anderson-L-Luiz (2025-05-11 18:09:36 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes from commit `c69d48e` in the provided diff likely achieve the following at a high level, summarizing the commits history:\n-\n-In summary:\n+\\newpage\n \n--- The previous commit (`bc90636`) introduces the need for creating directories and files necessary for the script.\n--- The latest commit (`c69d48e`) fulfills this need by setting up and installing required packages, creating necessary directories and files, and setting up and running the Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report.\n-\n-These changes likely have a high impact as they improve the functionality of the commit--log.tex file. This kind of pre--push hook could automate the process of creating a summary of the latest commit and saving it in a LaTeX report, streamlining the process of maintaining a commit log and potentially contributing to better record--keeping and collaboration within the team.\n-\n-The purpose of the changes could be any combination of the following:\n--- Automating the process of creating a summary of the latest commit and incorporating it into the LaTeX report.\n--- Creating log entries more efficiently, such as by pre--populating logs with automatically generated summaries.\n--- Monitoring changes to a software project by creating a record of each subtile change and contributing to version control.\n--- Ensuring that each commit can be tracked with a recent summary added to the log. This ensures that the logs are up--to--date.\n-\n-Hence, the changes seem to aim at generating a summary of the latest commit and adding it to the LaTeX report in a Git repository. It does this by making use of a vLLM API for generating these summaries, and the script also checks for any issues and logs error messages. Thus, the changes likely aim to create a more efficient and automated way of managing a commit log.\n-}\n-subsubsection*{Commit Details}\n-egin{itemize}\n-    item \textbf{Commit Hash:} 07a68b778a1003798118fa8e7833f5fd5a2947d4\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:09:36 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit f952688 by Anderson-L-Luiz (2025-05-11 18:22:43 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes in the commit f952688 for the file install_cum.sh likely modify the script's behavior upon completion.\n-\n-Before the changes, the script used an exit 0 command to indicate that the script had finished executing successfully, which may have been considered a standard way of indicating successful co\n\n[Diff truncated due to length]\n\nAssistant:", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:40:59] INFO async_llm.py:252: Added request chatcmpl-3b6f0a2a3cab4ed790c5c093c454d9aa.
[2025-05-12 05:41:03] INFO loggers.py:111: Engine 000: Avg prompt throughput: 149.1 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[2025-05-12 05:41:13] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:41:23] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:42:24] INFO logger.py:39: Received request chatcmpl-91e5b493acb6445babd08b6cadf210a9: prompt: "<｜begin▁of▁sentence｜>User: Summarize the following code changes from commit a5350b3. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/CUM_report/commit_log.tex b/CUM_report/commit_log.tex\nindex f5c5b46..7143ba5 100644\n--- a/CUM_report/commit_log.tex\n+++ b/CUM_report/commit_log.tex\n@@ -1,79 +1,26 @@\n \\documentclass{article}\n \\usepackage[utf8]{inputenc}\n-\\title{Commit Summary Report}\n-\\author{Generated by CUM Report}\n+\\usepackage{parskip}\n+\\usepackage{hyperref}\n+\\usepackage[T1]{fontenc}\n+\\usepackage{lmodern}\n+\\usepackage{amsmath}\n+\\usepackage{courier} % For pcr font family\n+\n+\\title{Commit Summary Report (CUM Report)}\n+\\author{Generated by Git Post-Commit Hook}\n \\date{\\today}\n+\n \\begin{document}\n \\maketitle\n+\\begin{abstract}\n+This document contains summaries of commits, generated automatically after each commit and included herein.\n+\\end{abstract}\n \\tableofcontents\n-\\section{Introduction}\n-This document contains summaries of commits in this repository.\n-section*{Commit c69d48e by Anderson-L-Luiz (2025-05-11 18:08:09 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes from commit c69d48e implement a Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report. The hook is triggered before each `git push` and ensures that a complete and recent report is available.\n-\n-The hook is designed to work with vLLM (a virtual language model) for generating summaries of commit changes. The script first installs the necessary packages (including vLLM's API client), then checks that the required directories and files exist. It then creates the hook file and installs the pre--push hook.\n-\n-The script proceeds to configure and run the vLLM API client to generate a summary for the latest commit. The summary is then inserted into the LaTeX report, and any error messages or issues are logged.\n-\n-Finally, the script ensures that the hook has been set up correctly and exits with a success status.\n-\n-The changes from the previous commit (bc90636) seem to be related to installation of required packages, creating required directories and files, and setting up and running the Git pre--push hook.\n-}\n-subsubsection*{Commit Details}\n-egin{itemize}\n-    item \textbf{Commit Hash:} c69d48e384e8ddfb4b33d0bc5d6a09646a809be9\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:08:09 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit 07a68b7 by Anderson-L-Luiz (2025-05-11 18:09:36 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes from commit `c69d48e` in the provided diff likely achieve the following at a high level, summarizing the commits history:\n-\n-In summary:\n+\\newpage\n \n--- The previous commit (`bc90636`) introduces the need for creating directories and files necessary for the script.\n--- The latest commit (`c69d48e`) fulfills this need by setting up and installing required packages, creating necessary directories and files, and setting up and running the Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report.\n-\n-These changes likely have a high impact as they improve the functionality of the commit--log.tex file. This kind of pre--push hook could automate the process of creating a summary of the latest commit and saving it in a LaTeX report, streamlining the process of maintaining a commit log and potentially contributing to better record--keeping and collaboration within the team.\n-\n-The purpose of the changes could be any combination of the following:\n--- Automating the process of creating a summary of the latest commit and incorporating it into the LaTeX report.\n--- Creating log entries more efficiently, such as by pre--populating logs with automatically generated summaries.\n--- Monitoring changes to a software project by creating a record of each subtile change and contributing to version control.\n--- Ensuring that each commit can be tracked with a recent summary added to the log. This ensures that the logs are up--to--date.\n-\n-Hence, the changes seem to aim at generating a summary of the latest commit and adding it to the LaTeX report in a Git repository. It does this by making use of a vLLM API for generating these summaries, and the script also checks for any issues and logs error messages. Thus, the changes likely aim to create a more efficient and automated way of managing a commit log.\n-}\n-subsubsection*{Commit Details}\n-egin{itemize}\n-    item \textbf{Commit Hash:} 07a68b778a1003798118fa8e7833f5fd5a2947d4\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:09:36 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit f952688 by Anderson-L-Luiz (2025-05-11 18:22:43 +0000)}\n-subsubsection*{AI Generated Summary}\n-{ontfamily{pcr}selectfont\n- The code changes in the commit f952688 for the file install_cum.sh likely modify the script's behavior upon completion.\n-\n-Before the changes, the script used an exit 0 command to indicate that the script had finished executing successfully, which may have been considered a standard way of indicating successful co\n\n[Diff truncated due to length]\n\nAssistant:", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:42:24] INFO async_llm.py:252: Added request chatcmpl-91e5b493acb6445babd08b6cadf210a9.
[2025-05-12 05:42:33] INFO loggers.py:111: Engine 000: Avg prompt throughput: 149.2 tokens/s, Avg generation throughput: 28.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[2025-05-12 05:42:43] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:42:53] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:45:16] INFO logger.py:39: Received request chatcmpl-a8f04b8e1233456f94d28bf10d8df410: prompt: '<｜begin▁of▁sentence｜>User: Summarize the following code changes from commit 184a5cc. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/install_cum.sh b/install_cum.sh\nindex 4c35e56..8273705 100644\n--- a/install_cum.sh\n+++ b/install_cum.sh\n@@ -210,7 +210,7 @@ fi\n \n sanitized_diff=$(echo "$diff_output_raw" | tr -d \'\\b\\f\') # Remove backspace & form-feed\n \n-MAX_DIFF_CHARS=5000 # Tunable: adjust if LLM is stable with more, or unstable with this\n+MAX_DIFF_CHARS=10000 # Tunable: adjust if LLM is stable with more, or unstable with this\n truncated_diff=$(echo "$sanitized_diff" | head -c "$MAX_DIFF_CHARS")\n \n diff_to_send="$truncated_diff"\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:45:16] INFO async_llm.py:252: Added request chatcmpl-a8f04b8e1233456f94d28bf10d8df410.
[2025-05-12 05:45:23] INFO loggers.py:111: Engine 000: Avg prompt throughput: 25.8 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:45:33] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:49:31] INFO logger.py:39: Received request chatcmpl-43470ff3ecd04107aaaaa692f2d57364: prompt: '<｜begin▁of▁sentence｜>User: Summarize the following code changes from commit f8c62d2. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/install_cum_bkp1.sh b/install_cum_bkp1.sh\nnew file mode 100644\nindex 0000000..1de0dd7\n--- /dev/null\n+++ b/install_cum_bkp1.sh\n@@ -0,0 +1,320 @@\n+#!/bin/bash\n+\n+# Strict mode\n+set -e # Exit immediately if a command exits with a non-zero status.\n+set -o pipefail # Causes a pipeline to return the exit status of the last command in the pipe that returned a non-zero return value.\n+\n+echo "🚀 Starting CUM Report installation and setup (using post-commit hook)..."\n+echo "Script execution started at: $(date)"\n+\n+# 1. Check if inside a Git repository and get the root directory\n+if ! git rev-parse --is-inside-work-tree > /dev/null 2>&1; then\n+  echo "❌ Error: This script must be run from within a Git repository."\n+  exit 1\n+fi\n+echo "Checkpoint 1: Git repository check passed."\n+\n+GIT_ROOT=$(git rev-parse --show-toplevel)\n+if [ -z "$GIT_ROOT" ]; then\n+    echo "❌ Error: Could not determine the root of the Git repository."\n+    exit 1\n+fi\n+echo "✅ Git repository root detected at: $GIT_ROOT"\n+echo "Checkpoint 2: Git root determined."\n+\n+# Function to install packages if missing\n+install_package() {\n+  local package="$1"\n+  if ! command -v "$package" &> /dev/null; then\n+    echo "🔧 $package not found. Attempting to install..."\n+    if command -v apt &> /dev/null; then\n+      sudo apt update && sudo apt install -y "$package"\n+    elif command -v yum &> /dev/null; then\n+      sudo yum install -y "$package"\n+    elif command -v brew &> /dev/null; then\n+      brew install "$package"\n+    else\n+      echo "❌ Error: No common package manager (apt, yum, brew) found. Please install $package manually."\n+      exit 1\n+    fi\n+\n+    if ! command -v "$package" &> /dev/null; then\n+         echo "❌ Error installing $package even after attempting. Please install it manually and rerun this script."\n+         exit 1\n+    fi\n+    echo "✅ $package installed successfully."\n+  else\n+    echo "👍 $package is already installed."\n+  fi\n+}\n+\n+install_package "jq"\n+install_package "curl"\n+echo "Checkpoint 3: Required packages checked/installed."\n+\n+CUM_REPORT_DIR="$GIT_ROOT/CUM_report"\n+echo "Checkpoint 4: About to ensure CUM_report directory exists at $CUM_REPORT_DIR."\n+mkdir -p "$CUM_REPORT_DIR"\n+if [ $? -ne 0 ]; then\n+    echo "❌ Error: Failed to create CUM_report directory at $CUM_REPORT_DIR. Check permissions."\n+    exit 1\n+fi\n+echo "✅ Ensured CUM_report directory exists at: $CUM_REPORT_DIR"\n+echo "Checkpoint 5: CUM_report directory ensured."\n+\n+INITIAL_TEX_FILE="$CUM_REPORT_DIR/commit_log.tex"\n+echo "Checkpoint 6: Checking for initial LaTeX file: $INITIAL_TEX_FILE."\n+if [ ! -f "$INITIAL_TEX_FILE" ]; then\n+  echo "📄 Creating initial LaTeX file: $INITIAL_TEX_FILE"\n+  cat <<EOF > "$INITIAL_TEX_FILE"\n+\\\\documentclass{article}\n+\\\\usepackage[utf8]{inputenc}\n+\\\\usepackage{parskip}\n+\\\\usepackage{hyperref}\n+\\\\usepackage[T1]{fontenc}\n+\\\\usepackage{lmodern}\n+\\\\usepackage{amsmath}\n+\\\\usepackage{courier}\n+\n+\\\\title{Commit Summary Report (CUM Report)}\n+\\\\author{Generated by Git Post-Commit Hook}\n+\\\\date{\\\\today}\n+\n+\\\\begin{document}\n+\\\\maketitle\n+\\\\begin{abstract}\n+This document contains summaries of commits, generated automatically after each commit and included herein.\n+\\\\end{abstract}\n+\\\\tableofcontents\n+\\\\newpage\n+\n+\\\\section{Introduction}\n+This document provides a log of commit summaries. Each section corresponds to a summary generated at the time of a commit.\n+The summaries are generated by an AI model based on the diff of the commit.\n+\n+\\\\end{document}\n+EOF\n+  if [ $? -ne 0 ]; then\n+    echo "❌ Error: Failed to create initial LaTeX file at $INITIAL_TEX_FILE."\n+    exit 1\n+  fi\n+  echo "✅ Initial LaTeX file created: $INITIAL_TEX_FILE"\n+else\n+  echo "👍 Initial LaTeX file already exists: $INITIAL_TEX_FILE"\n+fi\n+echo "Checkpoint 7: Initial LaTeX file status checked/created."\n+\n+# 4. Define and install the post-commit hook\n+HOOK_PATH="$GIT_ROOT/.git/hooks/post-commit" # CHANGED to post-commit\n+HOOK_DIR_PATH=$(dirname "$HOOK_PATH")\n+\n+# Remove old pre-push hook if it exists from previous installations of this tool\n+OLD_PRE_PUSH_HOOK_PATH="$GIT_ROOT/.git/hooks/pre-push"\n+if [ -f "$OLD_PRE_PUSH_HOOK_PATH" ]; then\n+    # Check if it\'s our hook by looking for a unique string, e.g., "CUM pre-push hook"\n+    # This is a simple check; more robust would be a hash or specific comment\n+    if grep -q "CUM pre-push hook" "$OLD_PRE_PUSH_HOOK_PATH"; then\n+        echo "ℹ️ Removing old CUM pre-push hook at $OLD_PRE_PUSH_HOOK_PATH."\n+        rm -f "$OLD_PRE_PUSH_HOOK_PATH"\n+    fi\n+fi\n+\n+echo "⚙️  Attempting to install post-commit hook to: $HOOK_PATH"\n+echo "Checkpoint 8: About to create hook directory $HOOK_DIR_PATH."\n+mkdir -p "$HOOK_DIR_PATH"\n+if [ $? -ne 0 ]; then\n+    echo "❌ CRITICAL Error: Failed to create hook directory at $HOOK_DIR_PATH. Check permissions."\n+    exit 1\n+fi\n+echo "✅ Hook directory ensured/created at $HOOK_DIR_PATH."\n+echo "Checkpoint 9: Hook directory creation attempt finished."\n+\n+echo\n\n[Diff truncated due to length]\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:49:31] INFO async_llm.py:252: Added request chatcmpl-43470ff3ecd04107aaaaa692f2d57364.
[2025-05-12 05:49:33] INFO loggers.py:111: Engine 000: Avg prompt throughput: 172.0 tokens/s, Avg generation throughput: 4.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[2025-05-12 05:49:43] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[2025-05-12 05:49:53] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:50:03] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:51:29] INFO logger.py:39: Received request chatcmpl-31230d926c184d56b6c847ffb45fae95: prompt: '<｜begin▁of▁sentence｜>User: Summarize the following code changes from commit 0005288. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/lorem.md b/lorem.md\nnew file mode 100644\nindex 0000000..1c1b531\n--- /dev/null\n+++ b/lorem.md\n@@ -0,0 +1,9 @@\n+Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer odio risus, congue eget ex rhoncus, laoreet aliquet arcu. Suspendisse lobortis sapien ultrices ante lacinia, id bibendum velit sodales. Aliquam tempus urna nisl, in rhoncus nibh aliquam vitae. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Etiam interdum suscipit risus, a vulputate enim consectetur sit amet. Phasellus venenatis magna eu bibendum maximus. Aliquam eleifend, nisi vitae elementum molestie, mi enim fringilla tortor, nec vestibulum nulla nunc a metus. Donec egestas diam sit amet nulla tempor, ac pellentesque massa condimentum. Aenean dapibus erat vel maximus gravida. Pellentesque in ex tortor.\n+\n+Quisque ac commodo mauris, gravida eleifend metus. Nulla ullamcorper, ex quis bibendum porttitor, sapien est tincidunt quam, in commodo nisi mi ac erat. In viverra diam ut mauris euismod, elementum ultrices lorem viverra. Phasellus at magna quis enim molestie elementum. Donec vel lacus sit amet turpis lacinia consectetur. In rhoncus lacus lorem, sit amet molestie nulla sollicitudin sit amet. Nulla sit amet metus rhoncus, vehicula mauris non, malesuada massa. Curabitur pulvinar aliquet interdum. In luctus lorem id accumsan sollicitudin. Curabitur commodo bibendum felis suscipit feugiat. Pellentesque vel gravida felis. Vivamus fermentum ex vel lacus ultricies tempus. Suspendisse pretium diam vel fermentum ullamcorper. Nam mollis interdum ex sit amet tristique.\n+\n+Donec lacus velit, aliquet ac risus sit amet, feugiat faucibus elit. Nulla sed leo id odio commodo lobortis. Donec eu dolor vitae neque porta ullamcorper et quis tellus. Curabitur fringilla quam eget auctor rhoncus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce vehicula velit libero, et mattis lacus vehicula vel. Nunc dolor quam, scelerisque nec ligula semper, ultricies pretium eros. Praesent vulputate massa ac nisl commodo, non aliquet elit eleifend.\n+\n+Proin sit amet quam odio. Morbi nec iaculis nibh, quis fermentum lacus. Suspendisse tellus lacus, rutrum non purus ac, fringilla elementum ligula. Phasellus pharetra lacinia lectus, ac aliquam velit pharetra id. Aliquam eu maximus mi. Pellentesque leo est, consectetur in neque non, placerat suscipit nisl. Proin elementum magna nec tellus hendrerit, sit amet tristique libero vulputate. Pellentesque eros felis, fringilla pellentesque lorem ut, vulputate consequat magna. Ut ut lacus urna. Etiam in porttitor dolor. Maecenas at pellentesque dolor, ut sodales sapien.\n+\n+Fusce id arcu lorem. Nulla elit lorem, tempor a varius at, tempor at libero. Pellentesque nulla ipsum, tempor at nibh a, egestas tincidunt ipsum. Nunc lectus diam, viverra quis consectetur sit amet, bibendum et urna. Aenean sodales euismod sem nec vestibulum. Nulla nec congue elit. Morbi nec nulla pretium sapien faucibus imperdiet sit amet et nisl.\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:51:29] INFO async_llm.py:252: Added request chatcmpl-31230d926c184d56b6c847ffb45fae95.
[2025-05-12 05:51:33] INFO loggers.py:111: Engine 000: Avg prompt throughput: 97.7 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
[2025-05-12 05:51:43] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:51:53] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
slurmstepd-ki-g0002: error: *** JOB 608 ON ki-g0002 CANCELLED AT 2025-05-12T06:05:26 ***
[2025-05-12 06:05:26] INFO launcher.py:79: Shutting down FastAPI HTTP server.
