Found ulimit of 10240 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=4096 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.
[2025-05-12 04:59:46] INFO core.py:58: Initializing a V1 LLM engine (v0.8.5.post1) with config: model='deepseek-ai/deepseek-moe-16b-chat', speculative_config=None, tokenizer='deepseek-ai/deepseek-moe-16b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/deepseek-moe-16b-chat, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
[2025-05-12 04:59:47] WARNING utils.py:2522: Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9cc264edd0>
[2025-05-12 04:59:47] INFO parallel_state.py:1004: rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[2025-05-12 04:59:47] INFO cuda.py:221: Using Flash Attention backend on V1 engine.
[2025-05-12 04:59:47] WARNING topk_topp_sampler.py:69: FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[2025-05-12 04:59:47] INFO gpu_model_runner.py:1329: Starting to load model deepseek-ai/deepseek-moe-16b-chat...
[2025-05-12 04:59:48] WARNING config.py:4122: `torch.compile` is turned on, but the model deepseek-ai/deepseek-moe-16b-chat does not support it. Please open an issue on GitHub if you want it to be supported.
[2025-05-12 04:59:48] INFO weight_utils.py:265: Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:05,  1.00it/s]
Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:02<00:05,  1.15s/it]
Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:03<00:04,  1.21s/it]
Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:04<00:03,  1.23s/it]
Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:05<00:02,  1.08s/it]
Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:06<00:01,  1.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.14s/it]

[2025-05-12 04:59:57] INFO loader.py:458: Loading weights took 8.33 seconds
[2025-05-12 04:59:57] INFO gpu_model_runner.py:1347: Model loading took 30.5301 GiB and 9.617664 seconds
[2025-05-12 04:59:57] WARNING fused_moe.py:668: Using default MoE config. Performance might be sub-optimal! Config file not found at /home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1408,device_name=NVIDIA_A100_80GB_PCIe.json
[2025-05-12 04:59:59] INFO kv_cache_utils.py:634: GPU KV cache size: 136,560 tokens
[2025-05-12 04:59:59] INFO kv_cache_utils.py:637: Maximum concurrency for 16,384 tokens per request: 8.33x
[2025-05-12 05:00:02] INFO gpu_model_runner.py:1686: Graph capturing finished in 4 secs, took 0.12 GiB
[2025-05-12 05:00:03] INFO core.py:159: init engine (profile, create kv cache, warmup model) took 5.31 seconds
[2025-05-12 05:00:03] INFO core_client.py:439: Core engine process 0 ready.
[2025-05-12 05:00:03] INFO api_server.py:1090: Starting vLLM API server on http://0.0.0.0:8001
[2025-05-12 05:00:03] INFO launcher.py:28: Available routes are:
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /openapi.json, Methods: HEAD, GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /docs, Methods: HEAD, GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /redoc, Methods: HEAD, GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /health, Methods: GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /load, Methods: GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /ping, Methods: POST, GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /tokenize, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /detokenize, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v1/models, Methods: GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /version, Methods: GET
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v1/chat/completions, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v1/completions, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v1/embeddings, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /pooling, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /score, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v1/score, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v1/audio/transcriptions, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /rerank, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v1/rerank, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /v2/rerank, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /invocations, Methods: POST
[2025-05-12 05:00:03] INFO launcher.py:36: Route: /metrics, Methods: GET
INFO:     Started server process [2541834]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
[2025-05-12 05:00:15] INFO chat_utils.py:397: Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2025-05-12 05:00:15] INFO logger.py:39: Received request chatcmpl-5ed24e92f7b145d49d0fa71b02301735: prompt: "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>User: Summarize the following code changes from commit 1190e72. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/CUM_report/commit_log.tex b/CUM_report/commit_log.tex\nindex f5c5b46..c943386 100644\n--- a/CUM_report/commit_log.tex\n+++ b/CUM_report/commit_log.tex\n@@ -1,79 +1,26 @@\n \\documentclass{article}\n \\usepackage[utf8]{inputenc}\n-\\title{Commit Summary Report}\n-\\author{Generated by CUM Report}\n+\\usepackage{parskip}\n+\\usepackage{hyperref}\n+\\usepackage[T1]{fontenc}\n+\\usepackage{lmodern} % Provides textasciitilde\n+\\usepackage{amsmath} % For general math, if \\sim is ever needed\n+\\usepackage{courier} % For \\fontfamily{pcr} (Courier)\n+\n+\\title{Commit Summary Report (CUM Report)}\n+\\author{Generated by Git Post-Commit Hook}\n \\date{\\today}\n+\n \\begin{document}\n \\maketitle\n+\\begin{abstract}\n+This document contains summaries of commits, generated automatically after each commit and included herein.\n+\\end{abstract}\n \\tableofcontents\n-\\section{Introduction}\n-This document contains summaries of commits in this repository.\n-section*{Commit c69d48e by Anderson-L-Luiz (2025-05-11 18:08:09 +0000)}\n-subsubsection*{AI Generated Summary}\n-{\x0contfamily{pcr}selectfont\n- The code changes from commit c69d48e implement a Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report. The hook is triggered before each `git push` and ensures that a complete and recent report is available.\n-\n-The hook is designed to work with vLLM (a virtual language model) for generating summaries of commit changes. The script first installs the necessary packages (including vLLM's API client), then checks that the required directories and files exist. It then creates the hook file and installs the pre--push hook.\n-\n-The script proceeds to configure and run the vLLM API client to generate a summary for the latest commit. The summary is then inserted into the LaTeX report, and any error messages or issues are logged.\n-\n-Finally, the script ensures that the hook has been set up correctly and exits with a success status.\n-\n-The changes from the previous commit (bc90636) seem to be related to installation of required packages, creating required directories and files, and setting up and running the Git pre--push hook.\n-}\n-subsubsection*{Commit Details}\n-\x08egin{itemize}\n-    item \textbf{Commit Hash:} c69d48e384e8ddfb4b33d0bc5d6a09646a809be9\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:08:09 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit 07a68b7 by Anderson-L-Luiz (2025-05-11 18:09:36 +0000)}\n-subsubsection*{AI Generated Summary}\n-{\x0contfamily{pcr}selectfont\n- The code changes from commit `c69d48e` in the provided diff likely achieve the following at a high level, summarizing the commits history:\n-\n-In summary:\n+\\newpage\n \n--- The previous commit (`bc90636`) introduces the need for creating directories and files necessary for the script.\n--- The latest commit (`c69d48e`) fulfills this need by setting up and installing required packages, creating necessary directories and files, and setting up and running the Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report.\n-\n-These changes likely have a high impact as they improve the functionality of the commit--log.tex file. This kind of pre--push hook could automate the process of creating a summary of the latest commit and saving it in a LaTeX report, streamlining the process of maintaining a commit log and potentially contributing to better record--keeping and collaboration within the team.\n-\n-The purpose of the changes could be any combination of the following:\n--- Automating the process of creating a summary of the latest commit and incorporating it into the LaTeX report.\n--- Creating log entries more efficiently, such as by pre--populating logs with automatically generated summaries.\n--- Monitoring changes to a software project by creating a record of each subtile change and contributing to version control.\n--- Ensuring that each commit can be tracked with a recent summary added to the log. This ensures that the logs are up--to--date.\n-\n-Hence, the changes seem to aim at generating a summary of the latest commit and adding it to the LaTeX report in a Git repository. It does this by making use of a vLLM API for generating these summaries, and the script also checks for any issues and logs error messages. Thus, the changes likely aim to create a more efficient and automated way of managing a commit log.\n-}\n-subsubsection*{Commit Details}\n-\x08egin{itemize}\n-    item \textbf{Commit Hash:} 07a68b778a1003798118fa8e7833f5fd5a2947d4\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:09:36 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit f952688 by Anderson-L-Luiz (2025-05-11 18:22:43 +0000)}\n-subsubsection*{AI Generated Summary}\n-{\x0contfamily{pcr}selectfont\n- The code changes in the commit f952688 for the file install_cum.sh likely modify the script's behavior upon completion.\n-\n-Before the changes, the script used an exit 0 command to indicate that the script had finished executing successfully, which may have been considered a standard way of indicating successful completion of a script in the context.\n-\n-After the changes, the exit 0 command is removed, and a new exit 0 command is added after a blank line. This new exit 0 command is not immediately after the original exit 0 command, but rather two lines below it. This means that the exit 0 command appears to be in a slightly different location in the file than before, which could mean that the script is behaving slightly differently after the changes.\n-\n-It's not clear exactly what the script does before and after these changes, but it's possible that the changes are related to error handling or reporting in some way. For example, the modified exit 0 command\n-}\n-subsubsection*{Commit Details}\n-\x08egin{itemize}\n-    item \textbf{Commit Hash:} f9526884542a233e19db3e8e985a674fbc94bf0a\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:22:43 +0000\n-end{itemize}\n-hrulefill\n+\\section{Introduction}\n+This document provides a log of commit summaries. Each section corresponds to a summary generated at the time of a commit.\n+The summaries are generated by an AI model based on the diff of the commit.\n \n \\end{document}\n\nAssistant:", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:00:15] INFO async_llm.py:252: Added request chatcmpl-5ed24e92f7b145d49d0fa71b02301735.
[2025-05-12 05:00:23] INFO loggers.py:111: Engine 000: Avg prompt throughput: 184.5 tokens/s, Avg generation throughput: 25.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[2025-05-12 05:00:33] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:00:43] INFO loggers.py:111: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2025-05-12 05:06:02] INFO logger.py:39: Received request chatcmpl-25280c133c7444b48b4713a1c7802fa8: prompt: '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>User: Summarize the following code changes from commit ad6136a. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\ndiff --git a/CUM_report/commit_log.tex b/CUM_report/commit_log.tex\nindex f5c5b46..7734b4f 100644\n--- a/CUM_report/commit_log.tex\n+++ b/CUM_report/commit_log.tex\n@@ -1,78 +1,54 @@\n \\documentclass{article}\n \\usepackage[utf8]{inputenc}\n-\\title{Commit Summary Report}\n-\\author{Generated by CUM Report}\n+\\usepackage{parskip}\n+\\usepackage{hyperref}\n+\\usepackage[T1]{fontenc}\n+\\usepackage{lmodern} % Provides textasciitilde\n+\\usepackage{amsmath} % For general math, if \\sim is ever needed\n+\\usepackage{courier} % For \\fontfamily{pcr} (Courier)\n+\n+\\title{Commit Summary Report (CUM Report)}\n+\\author{Generated by Git Post-Commit Hook}\n \\date{\\today}\n+\n \\begin{document}\n \\maketitle\n+\\begin{abstract}\n+This document contains summaries of commits, generated automatically after each commit and included herein.\n+\\end{abstract}\n \\tableofcontents\n-\\section{Introduction}\n-This document contains summaries of commits in this repository.\n-section*{Commit c69d48e by Anderson-L-Luiz (2025-05-11 18:08:09 +0000)}\n-subsubsection*{AI Generated Summary}\n-{\x0contfamily{pcr}selectfont\n- The code changes from commit c69d48e implement a Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report. The hook is triggered before each `git push` and ensures that a complete and recent report is available.\n-\n-The hook is designed to work with vLLM (a virtual language model) for generating summaries of commit changes. The script first installs the necessary packages (including vLLM\'s API client), then checks that the required directories and files exist. It then creates the hook file and installs the pre--push hook.\n-\n-The script proceeds to configure and run the vLLM API client to generate a summary for the latest commit. The summary is then inserted into the LaTeX report, and any error messages or issues are logged.\n+\\newpage\n \n-Finally, the script ensures that the hook has been set up correctly and exits with a success status.\n-\n-The changes from the previous commit (bc90636) seem to be related to installation of required packages, creating required directories and files, and setting up and running the Git pre--push hook.\n-}\n-subsubsection*{Commit Details}\n-\x08egin{itemize}\n-    item \textbf{Commit Hash:} c69d48e384e8ddfb4b33d0bc5d6a09646a809be9\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:08:09 +0000\n-end{itemize}\n-hrulefill\n+\\section{Introduction}\n+This document provides a log of commit summaries. Each section corresponds to a summary generated at the time of a commit.\n+The summaries are generated by an AI model based on the diff of the commit.\n \n-section*{Commit 07a68b7 by Anderson-L-Luiz (2025-05-11 18:09:36 +0000)}\n+section*{Commit 1190e72 by Anderson-L-Luiz (2025-05-12 04:59:39 +0000)}\n subsubsection*{AI Generated Summary}\n {\x0contfamily{pcr}selectfont\n- The code changes from commit `c69d48e` in the provided diff likely achieve the following at a high level, summarizing the commits history:\n+ Git Post--Commit Hook:\n \n-In summary:\n+These commits (1190e72) add a Git post--commit hook in the CUM\textbackslash{}_report repository. When a commit is pushed, the hook generates a short summary of the changes made in the last commit. The summary is written in a LaTeX report \textbackslash{}emph{commit\textbackslash{}_log.tex}.\n \n--- The previous commit (`bc90636`) introduces the need for creating directories and files necessary for the script.\n--- The latest commit (`c69d48e`) fulfills this need by setting up and installing required packages, creating necessary directories and files, and setting up and running the Git pre--push hook that generates a summary of the latest commit and adds it to a LaTeX report.\n+The commit results in an automated process for generating commit summaries. These summaries are printed in the commit\textbackslash{}_log.tex file along with a timestamp of when the commit was pushed. This could be used as a quick notation of what changes were made in a commit.\n \n-These changes likely have a high impact as they improve the functionality of the commit--log.tex file. This kind of pre--push hook could automate the process of creating a summary of the latest commit and saving it in a LaTeX report, streamlining the process of maintaining a commit log and potentially contributing to better record--keeping and collaboration within the team.\n+The hooks allow for automation and standardization of operations. Git post--commit hooks like these are useful in maintaining a complete record of all commits.\n \n-The purpose of the changes could be any combination of the following:\n--- Automating the process of creating a summary of the latest commit and incorporating it into the LaTeX report.\n--- Creating log entries more efficiently, such as by pre--populating logs with automatically generated summaries.\n--- Monitoring changes to a software project by creating a record of each subtile change and contributing to version control.\n--- Ensuring that each commit can be tracked with a recent summary added to the log. This ensures that the logs are up--to--date.\n+Git Pre--Push Hook:\n \n-Hence, the changes seem to aim at generating a summary of the latest commit and adding it to the LaTeX report in a Git repository. It does this by making use of a vLLM API for generating these summaries, and the script also checks for any issues and logs error messages. Thus, the changes likely aim to create a more efficient and automated way of managing a commit log.\n-}\n-subsubsection*{Commit Details}\n-\x08egin{itemize}\n-    item \textbf{Commit Hash:} 07a68b778a1003798118fa8e7833f5fd5a2947d4\n-    item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:09:36 +0000\n-end{itemize}\n-hrulefill\n-\n-section*{Commit f952688 by Anderson-L-Luiz (2025-05-11 18:22:43 +0000)}\n-subsubsection*{AI Generated Summary}\n-{\x0contfamily{pcr}selectfont\n- The code changes in the commit f952688 for the file install_cum.sh likely modify the script\'s behavior upon completion.\n+The following changes (1190e72) add a pre--push Git hook in the CUM\textbackslash{}_report repository. When pushing a commit, the hook is triggered, generates a summary of the latest commit, and adds this summary to the LaTeX report in a Git repository.\n \n-Before the changes, the script used an exit 0 command to indicate that the script had finished executing successfully, which may have been considered a standard way of indicating successful completion of a script in the context.\n+The hook scripts are designed to work with a virtual language model called vLLM for generating summaries of commit changes. The vLLM API client is installed, and the directories and files required for the script are created.\n \n-After the changes, the exit 0 command is removed, and a new exit 0 command is added after a blank line. This new exit 0 command is not immediately after the original exit 0 command, but rather two lines below it. This means that the exit 0 command appears to be in a slightly different location in the file than before, which could mean that the script is behaving slightly differently after the changes.\n+The script then checks for the required directories and files. Then it creates the hook file and installs the pre--push hook. The script runs the vLLM API client to generate a summary for the latest commit. The summary text is inserted into the LaTeX report and any error messages or issues are logged. The hook checks if the hook has been set up correctly and exists with a success message.\n \n-It\'s not clear exactly what the script does before and after these changes, but it\'s possible that the changes are related to error handling or reporting in some way. For example, the modified exit 0 command\n+The pre--push hook adds a level of automation to the generation and application of summaries of last commits. The vLLM API client allows for the generation of summaries in a standardized, AI--assisted way, which can be beneficial for keeping records of commits, tracking changes, and aiding team collaboration.\n }\n subsubsection*{Commit Details}\n \x08egin{itemize}\n-    item \textbf{Commit Hash:} f9526884542a233e19db3e8e985a674fbc94bf0a\n+    item \textbf{Commit Hash:} 1190e72a12d1e85e60b33f146e2912be7ba0c34f\n     item \textbf{Author:} Anderson-L-Luiz\n-    item \textbf{Date:} 2025-05-11 18:22:43 +0000\n+    item \textbf{Date:} 2025-05-12 04:59:39 +0000\n end{itemize}\n hrulefill\n \ndiff --git a/install_cum.sh b/install_cum.sh\nindex 1de0dd7..4e710d7 100644\n--- a/install_cum.sh\n+++ b/install_cum.sh\n@@ -33,12 +33,15 @@ install_package() {\n       sudo yum install -y "$package"\n     elif command -v brew &> /dev/null; then\n       brew install "$package"\n+    elif command -v perl &> /dev/null && [ "$package" == "perl" ]; then \n+        echo "üëç Perl is already installed (checked as a dependency)."\n+        return 0\n     else\n       echo "‚ùå Error: No common package manager (apt, yum, brew) found. Please install $package manually."\n       exit 1\n     fi\n \n-    if ! command -v "$package" &> /dev/null; then\n+    if ! command -v "$package" &> /dev/null && ! (command -v perl &> /dev/null && [ "$package" == "perl" ]); then\n          echo "‚ùå Error installing $package even after attempting. Please install it manually and rerun this script."\n          exit 1\n     fi\n@@ -50,6 +53,7 @@ install_package() {\n \n install_package "jq"\n install_package "curl"\n+install_package "perl" \n echo "Checkpoint 3: Required packages checked/installed."\n \n CUM_REPORT_DIR="$GIT_ROOT/CUM_report"\n@@ -72,9 +76,9 @@ if [ ! -f "$INITIAL_TEX_FILE" ]; then\n \\\\usepackage{parskip}\n \\\\usepackage{hyperref}\n \\\\usepackage[T1]{fontenc}\n-\\\\usepackage{lmodern}\n-\\\\usepackage{amsmath}\n-\\\\usepackage{courier}\n+\\\\usepackage{lmodern} \n+\\\\usepackage{amsmath} \n+\\\\usepackage{courier} \n \n \\\\title{Commit Summary Report (CUM Report)}\n \\\\author{Generated by Git Post-Commit Hook}\n@@ -104,16 +108,12 @@ else\n fi\n echo "Checkpoint 7: Initial LaTeX file status checked/created."\n \n-# 4. Define and install the post-commit hook\n-HOOK_PATH="$GIT_ROOT/.git/hooks/post-commit" # CHANGED to post-commit\n+HOOK_PATH="$GIT_ROOT/.git/hooks/post-commit"\n HOOK_DIR_PATH=$(dirname "$HOOK_PATH")\n \n-# Remove old pre-push hook if it exists from previous installations of this tool\n OLD_PRE_PUSH_HOOK_PATH="$GIT_ROOT/.git/hooks/pre-push"\n if [ -f "$OLD_PRE_PUSH_HOOK_PATH" ]; then\n-    # Check if it\'s our hook by looking for a unique string, e.g., "CUM pre-push hook"\n-    # This is a simple check; more robust would be a hash or specific comment\n-    if grep -q "CUM pre-push hook" "$OLD_PRE_PUSH_HOOK_PATH"; then\n+    if grep -q "CUM pre-push hook" "$OLD_PRE_PUSH_HOOK_PATH" || grep -q "VLLM_API_URL" "$OLD_PRE_PUSH_HOOK_PATH"; then\n         echo "‚ÑπÔ∏è Removing old CUM pre-push hook at $OLD_PRE_PUSH_HOOK_PATH."\n         rm -f "$OLD_PRE_PUSH_HOOK_PATH"\n     fi\n@@ -133,12 +133,10 @@ echo "Checkpoint 10: About to write post-commit hook content to $HOOK_PATH."\n cat << \'HOOK_SCRIPT_EOF\' > "$HOOK_PATH"\n #!/bin/bash\n \n-# Prevent the hook from running when we are amending the commit from within this hook\n if [ "$GIT_CUM_REPORT_AMENDING_COMMIT" = "true" ]; then\n   exit 0\n fi\n \n-# Hook script strict mode\n set -e \n set -o pipefail\n \n@@ -147,18 +145,15 @@ echo "üî• Running CUM post-commit hook..."\n GIT_ROOT_DIR=$(git rev-parse --show-toplevel)\n if [ -z "$GIT_ROOT_DIR" ]; then\n     echo "‚ùå Hook Error: Could not determine Git root directory. Aborting hook." >&2\n-    exit 1 # Should not happen in a post-commit hook context\n+    exit 1 \n fi\n TEX_FILE_PATH="$GIT_ROOT_DIR/CUM_report/commit_log.tex"\n \n if [ ! -f "$TEX_FILE_PATH" ]; then\n     echo "‚ùå Hook Error: LaTeX log file not found at $TEX_FILE_PATH." >&2\n-    echo "Please ensure it exists or re-run the CUM Report installation script." >&2\n-    # Don\'t fail the commit for this, but log it.\n     exit 0\n fi\n \n-# For post-commit, HEAD is the commit that was just made\n commit_hash=$(git rev-parse HEAD)\n commit_short_hash=$(git rev-parse --short HEAD)\n commit_author=$(git log -1 --pretty=format:\'%an\' HEAD)\n@@ -166,43 +161,76 @@ commit_date=$(git log -1 --pretty=format:\'%ad\' --date=iso-local HEAD)\n \n echo "üîé Analyzing commit: $commit_short_hash by $commit_author on $commit_date"\n \n-# Get the diff of the commit just made (HEAD) against its parent (HEAD^)\n-# For the initial commit which has no parent, this will error. Handle it.\n if git rev-parse --verify HEAD^ >/dev/null 2>&1; then\n-    diff_output=$(git diff HEAD^ HEAD --pretty=format:) # Diff against parent\n+    diff_output=$(git diff HEAD^ HEAD --pretty=format:) \n else\n-    # This is the initial commit, show its full tree\n     diff_output=$(git show --pretty=format: HEAD)\n fi\n \n-\n if [ -z "$diff_output" ]; then\n-  echo "‚ö†Ô∏è No textual changes found in commit $commit_short_hash to summarize (or initial commit structure). Skipping API call."\n-  # If it\'s an empty commit or just the initial commit before any real files, don\'t try to summarize.\n+  echo "‚ö†Ô∏è No textual changes found in commit $commit_short_hash to summarize. Skipping API call."\n   exit 0 \n fi\n \n-prompt_text="Summarize the following code changes from commit $commit_short_hash. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\n$diff_output"\n+# --- MODIFIED SECTION START: Truncate diff and prepare prompt ---\n+# Max characters for the diff part of the prompt. Adjust as needed.\n+# Deepseek MOE 16K context window. 1 token ~4 chars. 20k chars ~5k tokens. This leaves ample room for other prompt text.\n+MAX_DIFF_CHARS=20000 \n+diff_len=${#diff_output}\n+is_truncated=false\n+\n+if [ "$diff_len" -gt "$MAX_DIFF_CHARS" ]; then\n+    echo "‚ö†Ô∏è Diff output is too long ($diff_len chars), truncating to $MAX_DIFF_CHARS chars for LLM prompt." >&2\n+    # Using printf and head -c for truncation. Note: head -c might cut mid-multi-byte char.\n+    # For extreme robustness, a more sophisticated truncation (e.g., by lines or token-aware) might be needed.\n+    diff_to_send=$(printf "%s" "$diff_output" | head -c $MAX_DIFF_CHARS)\n+    is_truncated=true\n+else\n+    diff_to_send="$diff_output"\n+fi\n+\n+if [ "$is_truncated" = true ]; then\n+    prompt_text="Summarize the following TRUNCATED code changes (original was $diff_len chars, showing first $MAX_DIFF_CHARS) from commit $commit_short_hash. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\n$diff_to_send\\n\\n[... Diff Truncated ...]"\n+else\n+    prompt_text="Summarize the following code changes from commit $commit_short_hash. Explain what the changes likely achieve at a high level and their potential impact or purpose:\\n\\n$diff_to_send"\n+fi\n+# --- MODIFIED SECTION END ---\n \n VLLM_API_URL="http://10.16.246.2:8001/v1/chat/completions"\n MODEL_NAME="deepseek-ai/deepseek-moe-16b-chat" \n-MAX_TOKENS=700\n+MAX_TOKENS=700 # Max *output* tokens\n+\n+echo "üìû Contacting LLM API at $VLLM_API_URL for summary. Approx prompt length (chars): ${#prompt_text}"\n+\n+payload_jq_script=\'\n+{\n+  model: $model_name,\n+  messages: [\n+    {role: "user", content: $user_prompt}\n+  ],\n+  max_tokens: $max_tokens_int,\n+  stream: false\n+}\'\n+\n+# --- Optional Debugging: Uncomment to see the exact payload ---\n+# TEMP_PAYLOAD=$(jq -n \\\n+#             --arg model_name "$MODEL_NAME" \\\n+#             --arg user_prompt "$prompt_text" \\\n+#             --argjson max_tokens_int "$MAX_TOKENS" \\\n+#             "$payload_jq_script")\n+# echo "--- DEBUG HOOK: Payload to be sent ---" >&2\n+# printf "%s\\n" "$TEMP_PAYLOAD" >&2\n+# echo "--- DEBUG HOOK END ---" >&2\n+# --- End Optional Debugging ---\n \n-echo "üìû Contacting LLM API at $VLLM_API_URL for summary..."\n payload=$(jq -n \\\n             --arg model_name "$MODEL_NAME" \\\n             --arg user_prompt "$prompt_text" \\\n-            --argjson max_tokens "$MAX_TOKENS" \\\n-            \'{\n-              model: $model_name,\n-              messages: [\n-                {role: "user", content: $user_prompt}\n-              ],\n-              max_tokens: $max_tokens,\n-              stream: false\n-            }\')\n+            --argjson max_tokens_int "$MAX_TOKENS" \\\n+            "$payload_jq_script")\n \n-if [ $? -ne 0 ]; then\n+\n+if [ $? -ne 0 ]; then # This checks jq\'s exit status\n     echo "‚ùå Hook Error: Failed to construct JSON payload with jq." >&2\n     exit 0 \n fi\n@@ -210,8 +238,8 @@ fi\n api_response=$(curl --silent --fail --show-error -X POST \\\n   -H "Content-Type: application/json" \\\n   -d "$payload" \\\n-  --connect-timeout 15 \\\n-  --max-time 120 \\\n+  --connect-timeout 20 \\ # Slightly increased connect timeout\n+  --max-time 180 \\      # Increased max time for potentially long summaries\n   "$VLLM_API_URL")\n curl_exit_status=$?\n \n@@ -219,22 +247,29 @@ summary_text=""\n \n if [ $curl_exit_status -ne 0 ]; then\n   echo "‚ùå Hook Warning: Failed to connect to LLM API or API returned an error (curl code: $curl_exit_status)." >&2\n-  echo "   URL: $VLLM_API_URL. Commit will proceed without summary." >&2\n+  echo "   URL: $VLLM_API_URL." >&2\n+  echo "   This could be due to: incorrect API endpoint, network issues, API server down/crashing, or the payload being rejected (check server logs)." >&2\n   summary_text="Automated summary generation failed: LLM API request error (curl code: $curl_exit_status)."\n else\n   extracted_text=$(echo "$api_response" | jq -r \'.choices[0].message.content\') \n-\n   if [ "$extracted_text" != "null" ] && [ -n "$extracted_text" ]; then\n     summary_text="$extracted_text"\n     echo -e "‚úÖ Commit summary received from LLM."\n   else\n     echo "‚ùå Hook Warning: Failed to extract summary from LLM response or response was empty." >&2\n-    echo "   Raw API Response (first 500 chars): $(echo "$api_response" | head -c 500)" >&2\n+    echo "   Attempted JQ path: .choices[0].message.content" >&2\n+    echo "   Raw API Response (first 500 chars): $(printf "%s" "$api_response" | head -c 500)" >&2\n     summary_text="Automated summary generation failed: Could not parse LLM response or response was empty."\n   fi\n fi\n \n-escaped_summary=$(echo "$summary_text" | sed \\\n+if command -v perl &> /dev/null; then\n+    summary_text_cleaned=$(printf "%s" "$summary_text" | perl -pe \'s/[^\\t\\n\\r -~]//g\') \n+else\n+    summary_text_cleaned=$(printf "%s" "$summary_text" | tr -d \'\\f\\b\\000-\\031\\177-\\377\' | tr -dc \'[:print:][:space:]\\t\\n\\r\')\n+fi\n+\n+escaped_summary=$(printf "%s" "$summary_text_cleaned" | sed \\\n   -e \'s/\\\\/\\\\textbackslash{}/g\' \\\n   -e \'s/{/\\\\{/g\'   -e \'s/}/\\\\}/g\' \\\n   -e \'s/\\$/\\\\\\$/g\' -e \'s/&/\\\\&/g\' \\\n@@ -244,8 +279,16 @@ escaped_summary=$(echo "$summary_text" | sed \\\n   -e \'s/</\\\\textless{}/g\' -e \'s/>/\\\\textgreater{}/g\' \\\n   -e \'s/-/--/g\')\n \n-section_title="Commit $commit_short_hash by $(echo "$commit_author" | sed \'s/[&%$_#]/\\_&/g; s/_/\\\\_/g\') ($(echo "$commit_date" | sed \'s/_/\\\\_/g\'))"\n+escape_latex_for_title() {\n+    printf "%s" "$1" | sed \\\n+        -e \'s/\\\\/\\\\textbackslash{}/g\' -e \'s/&/\\\\&/g\' -e \'s/%/\\\\%/g\' \\\n+        -e \'s/\\$/\\\\\\$/g\' -e \'s/_/\\\\_/g\' -e \'s/#/\\\\#/g\' \\\n+        -e \'s/{/\\\\{/g\'  -e \'s/}/\\\\}/g\'\n+}\n+escaped_author=$(escape_latex_for_title "$commit_author")\n+escaped_date=$(escape_latex_for_title "$commit_date")\n \n+section_title="Commit $commit_short_hash by $escaped_author ($escaped_date)"\n section_content="\\\\section*{$section_title}\n \\\\subsubsection*{AI Generated Summary}\n {\\\\fontfamily{pcr}\\\\selectfont\n@@ -254,11 +297,12 @@ $escaped_summary\n \\\\subsubsection*{Commit Details}\n \\\\begin{itemize}\n     \\\\item \\\\textbf{Commit Hash:} $commit_hash\n-    \\\\item \\\\textbf{Author:} $(echo "$commit_author" | sed \'s/[&%$_#]/\\_&/g; s/_/\\\\_/g\')\n-    \\\\item \\\\textbf{Date:} $(echo "$commit_date" | sed \'s/_/\\\\_/g\')\n+    \\\\item \\\\textbf{Author:} $escaped_author\n+    \\\\item \\\\textbf{Date:} $escaped_date\n \\\\end{itemize}\n \\\\hrulefill\n "\n+\n TMP_TEX_FILE="$GIT_ROOT_DIR/CUM_report/tmp_commit_log.$$.tex"\n awk -v section="$section_content" \'/\\\\end\\{document\\}/{print section}1\' "$TEX_FILE_PATH" > "$TMP_TEX_FILE"\n awk_status=$?\n@@ -269,9 +313,8 @@ if [ $awk_status -eq 0 ]; then\n         echo "‚úÖ Summary for commit $commit_short_hash appended to $TEX_FILE_PATH"\n         echo " amending commit to include this change..."\n         git add "$TEX_FILE_PATH"\n-        # Set a flag to prevent re-entry into this hook\n         export GIT_CUM_REPORT_AMENDING_COMMIT=true\n-        git commit --amend --no-edit -C HEAD # Use -C HEAD to reuse original commit message, author, date\n+        git commit --amend --no-edit -C HEAD \n         unset GIT_CUM_REPORT_AMENDING_COMMIT\n         echo "‚úÖ Commit amended to include TeX log update."\n     else\n@@ -279,7 +322,7 @@ if [ $awk_status -eq 0 ]; then\n         rm -f "$TMP_TEX_FILE" \n     fi\n else\n-    echo "‚ùå Hook Warning: Failed to write summary to TeX file using awk. TeX log not updated in commit." >&2\n+    echo "‚ùå Hook Warning: Failed to write summary to TeX file using awk (awk exit code: $awk_status). Temp file: $TMP_TEX_FILE" >&2\n     rm -f "$TMP_TEX_FILE" \n fi\n \n@@ -311,10 +354,7 @@ echo "Checkpoint 13: Hook made executable."\n \n echo "üéâ CUM Report setup complete with a post-commit hook!"\n echo "After each \'git commit\', the summary will be generated and the commit will be amended to include the changes to \'$INITIAL_TEX_FILE\'."\n-echo "Your working directory should remain clean regarding this file after a commit."\n echo "To view the report, compile \'$INITIAL_TEX_FILE\' (e.g., using \'pdflatex $INITIAL_TEX_FILE\' in the \'$CUM_REPORT_DIR\' directory)."\n-echo ""\n-echo "If the hook doesn\'t run after \'git commit\', check \'git config core.hooksPath\'."\n echo "Script execution finished at: $(date)"\n \n-exit 0 \n\\ No newline at end of file\n+exit 0\n\\ No newline at end of file\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=700, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
[2025-05-12 05:06:02] INFO async_llm.py:252: Added request chatcmpl-25280c133c7444b48b4713a1c7802fa8.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [204,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
[2025-05-12 05:06:02] ERROR core.py:398: EngineCore encountered a fatal error.
Traceback (most recent call last):
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 389, in run_engine_core
    engine_core.run_busy_loop()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 413, in run_busy_loop
    self._process_engine_step()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 438, in _process_engine_step
    outputs = self.step_fn()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 203, in step
    output = self.model_executor.execute_model(scheduler_output)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 86, in execute_model
    output = self.collective_rpc("execute_model",
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 268, in execute_model
    output = self.model_runner.execute_model(scheduler_output)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1092, in execute_model
    output = self.model(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/deepseek.py", line 468, in forward
    hidden_states = self.model(input_ids, positions, intermediate_tensors,
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/deepseek.py", line 379, in forward
    hidden_states, residual = layer(positions, hidden_states, residual)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/deepseek.py", line 317, in forward
    hidden_states, residual = self.input_layernorm(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/custom_op.py", line 25, in forward
    return self._forward_method(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 120, in forward_native
    x = x + residual.to(torch.float32)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Process EngineCore_0:
[2025-05-12 05:06:02] ERROR async_llm.py:399: AsyncLLM output_handler failed.
Traceback (most recent call last):
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py", line 357, in output_handler
    outputs = await engine_core.get_output_async()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 716, in get_output_async
    raise self._format_exception(outputs) from None
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
[2025-05-12 05:06:02] INFO async_llm.py:324: Request chatcmpl-25280c133c7444b48b4713a1c7802fa8 failed (engine dead).
Traceback (most recent call last):
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 389, in run_engine_core
    engine_core.run_busy_loop()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 413, in run_busy_loop
    self._process_engine_step()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 438, in _process_engine_step
    outputs = self.step_fn()
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 203, in step
    output = self.model_executor.execute_model(scheduler_output)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 86, in execute_model
    output = self.collective_rpc("execute_model",
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 268, in execute_model
    output = self.model_runner.execute_model(scheduler_output)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1092, in execute_model
    output = self.model(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/deepseek.py", line 468, in forward
    hidden_states = self.model(input_ids, positions, intermediate_tensors,
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/deepseek.py", line 379, in forward
    hidden_states, residual = layer(positions, hidden_states, residual)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/deepseek.py", line 317, in forward
    hidden_states, residual = self.input_layernorm(
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/custom_op.py", line 25, in forward
    return self._forward_method(*args, **kwargs)
  File "/home/aimotion_api/anaconda3/envs/seraphim_vllm_env/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py", line 120, in forward_native
    x = x + residual.to(torch.float32)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [176,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [2541834]
